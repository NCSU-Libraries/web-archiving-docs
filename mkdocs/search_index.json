{
    "docs": [
        {
            "location": "/", 
            "text": "Web Archiving at NCSU Libraries\n\n\nThe \nSpecial Collections Research Center\n (SCRC) at NCSU Libraries has established a Web Archive with the primary purpose of documenting for historic record the ever-changing web landscape of both official websites of NC State University as well as websites that complement the existing SCRC \ncollecting areas\n and collections. In order to facilitate our web archiving project we have partnered with the \nInternet Archive\n. The Internet Archive provides the tools and technology required to crawl, capture, store, and provide access to the websites in our collection. You can view all of our public collections by visiting \nhttps://archive-it.org/home/ncsu-libraries\n\n\nDescription of Collections\n\n\nNC State University Web Sites\n\n\nThe NCSU Web Archive is partially dedicated to preserving the unique cultural heritage of our institution as it is represented on university websites.  We have identified most of the primary websites used by the university and are regularly capturing those sites for preservation and future access. These websites represent both large campus departments, individual colleges as well as departments and other organizations within those colleges.\n\n\nJames B. Hunt Jr. Library Impact Collection\n\n\nThe James B. Hunt Jr. Library Impact collection is dedicated to documenting the impact that the \nJames B. Hunt library\n has had on our students, our community and the greater library community as a whole. The web pages included in this collection are primarily articles that have been written about the library. As new articles are published they will be added to the collection.\n\n\nLandscape Architecture\n\n\nThe Landscape Architecture Web Archive is meant to complement the Special Collection Research Center's existing \nLandscape Architecture collections\n. It contains websites of prominent landscape architects, landscape architecture firms as well as websites dedicated to large landscape architecture projects.\n\n\nArchitecture and Design\n\n\nThe Architecture and Design Web Archive collection is meant to complement the Special Collection Research Center's \nArchitecture and Design collections\n. It features websites from top architectural and design firms in the region, as well as websites of award-winning architects.\n\n\nWeb Archive Management\n\n\nPermissions Model\n\n\nMany of the websites that we archive are official NC State University websites that are part of the official records of the university and are required to be included in our University Archives. For most non-NCSU websites that are related to our existing collections we contact the content owner to request permission to archive the site.\n\n\nTakedown Requests\n\n\nNCSU Libraries has established a takedown request policy both for requests generated within the NCSU domain as well as for sites outside of the NCSU domain. Takedown requests from within the NCSU domain must come from the Office of the General Counsel. In the event that the owner of a website that is outside of the NCSU domain wishes for content to be removed they must notify the Libraries.", 
            "title": "Home"
        }, 
        {
            "location": "/#web-archiving-at-ncsu-libraries", 
            "text": "The  Special Collections Research Center  (SCRC) at NCSU Libraries has established a Web Archive with the primary purpose of documenting for historic record the ever-changing web landscape of both official websites of NC State University as well as websites that complement the existing SCRC  collecting areas  and collections. In order to facilitate our web archiving project we have partnered with the  Internet Archive . The Internet Archive provides the tools and technology required to crawl, capture, store, and provide access to the websites in our collection. You can view all of our public collections by visiting  https://archive-it.org/home/ncsu-libraries", 
            "title": "Web Archiving at NCSU Libraries"
        }, 
        {
            "location": "/#description-of-collections", 
            "text": "", 
            "title": "Description of Collections"
        }, 
        {
            "location": "/#nc-state-university-web-sites", 
            "text": "The NCSU Web Archive is partially dedicated to preserving the unique cultural heritage of our institution as it is represented on university websites.  We have identified most of the primary websites used by the university and are regularly capturing those sites for preservation and future access. These websites represent both large campus departments, individual colleges as well as departments and other organizations within those colleges.", 
            "title": "NC State University Web Sites"
        }, 
        {
            "location": "/#james-b-hunt-jr-library-impact-collection", 
            "text": "The James B. Hunt Jr. Library Impact collection is dedicated to documenting the impact that the  James B. Hunt library  has had on our students, our community and the greater library community as a whole. The web pages included in this collection are primarily articles that have been written about the library. As new articles are published they will be added to the collection.", 
            "title": "James B. Hunt Jr. Library Impact Collection"
        }, 
        {
            "location": "/#landscape-architecture", 
            "text": "The Landscape Architecture Web Archive is meant to complement the Special Collection Research Center's existing  Landscape Architecture collections . It contains websites of prominent landscape architects, landscape architecture firms as well as websites dedicated to large landscape architecture projects.", 
            "title": "Landscape Architecture"
        }, 
        {
            "location": "/#architecture-and-design", 
            "text": "The Architecture and Design Web Archive collection is meant to complement the Special Collection Research Center's  Architecture and Design collections . It features websites from top architectural and design firms in the region, as well as websites of award-winning architects.", 
            "title": "Architecture and Design"
        }, 
        {
            "location": "/#web-archive-management", 
            "text": "", 
            "title": "Web Archive Management"
        }, 
        {
            "location": "/#permissions-model", 
            "text": "Many of the websites that we archive are official NC State University websites that are part of the official records of the university and are required to be included in our University Archives. For most non-NCSU websites that are related to our existing collections we contact the content owner to request permission to archive the site.", 
            "title": "Permissions Model"
        }, 
        {
            "location": "/#takedown-requests", 
            "text": "NCSU Libraries has established a takedown request policy both for requests generated within the NCSU domain as well as for sites outside of the NCSU domain. Takedown requests from within the NCSU domain must come from the Office of the General Counsel. In the event that the owner of a website that is outside of the NCSU domain wishes for content to be removed they must notify the Libraries.", 
            "title": "Takedown Requests"
        }, 
        {
            "location": "/admin/", 
            "text": "Archive-It Admin Interface\n\n\nNCSU Libraries uses the Archive-It web archiving service to create our web archives. This service, provided by the \nInternet Archive\n, allows us to manage the crawl, capture and storage of websites. Archive-It is a subscription service with an annual fee. To learn more about Archive-It and the Internet Archive you can visit \nhttps://archive.org/about/\n.\n\n\nAccess Restrictions / User Roles\n\n\nThe Archive-It administrator interface can be accessed by logging into \nhttps://partner.archive-it.org/archiveit\n. In order to log in you need to be added as an administrator for our account. There are three levels of access that can be granted:\n\n\n\n\nAdministrator - Full administrative privileges. Can add additional users and manage all aspects of the account.\n\n\nUser - Can adjust all aspects of the collection, but are not administrators (can not add users)\n\n\nContributor - Lowest level of privileges. They can access the account and see crawl reports, but cannot add or remove seeds, start or stop crawls. This setting is good for staff members conducting QA.\n\n\n\n\n\n\nMain Administrative Functions\n\n\nThis is a brief overview of each of the areas available within the admin interface. For more in-depth documentation please visit the \nArchive-It Help Center\n. The links below link to our Archive-It administration pages so a valid Archive-It login is required.\n\n\nHome\n\n\nWhen you first log in to the administrative interface you will see the home screen. This screen displays a list of your collections as well as a graph showing your current data usage. Currently we are allowed 1TB of data per subscription cycle (February - February). This page also displays data collection rates for each individual collection for the current subscription period. It is a good way to monitor the data usage, number of seed URLs and recent crawl dates for all of your collections.\n\n\n\n\n\n\n\n\nCollections\n\n\nThe collections page lists all collections within our account. We base our collections around our current collecting areas. Our primary collection is our NC State University Websites. From this page you can create a new collection, or monitor existing collections. The collections page shows current data, all time data and the number of active seeds for each collection. It is important to note that current data is data collected during the current one-year subscription.\n\n\n\n\n\n\n\n\nCrawls\n\n\nThe crawls tab provides access to crawl reports, a list of current crawls, a list of scheduled crawls as well as a way to compare two crawls to one another. You can download lists of crawl reports or individual items from within a crawl report from this area. This area also provides a search box allowing you to search by Crawl ID, Status or Frequency.\n\n\n\n\n\n\n\n\n\n\n\n\nCurrent Crawls\n:\nThe current crawls lists every crawl during the current subscription period for your account.\n\n\n\n\n\n\nTest Crawls\n:\nThe test crawls tab filters all of the crawl reports to only show reports that pertain to test crawls. This allows you to narrow down the list to see only completed test crawls. These reports are useful when you begin to work on scoping new seeds, and run test crawls to test what the captured data will look like.\n\n\n\n\n\n\nScheduled Crawls\n\nThe scheduled crawls tab shows a list of each collection you have and what crawls are scheduled to run for those collections. It also notes whether or not you have set crawl data limits for any of those regularly scheduled crawls.\n\n\n\n\n\n\nCrawl Reports\n\n\nThe crawl reports tab shows a full list of all completed crawls. This includes completed regularly scheduled crawls as well as test and patch crawls. Click on the crawl ID to open an individual crawl report. Individual crawl reports show an overview of the data collected during that crawl. In addition to overall data numbers the following can be accessed from a crawl report:\n\n\n\n\nSeeds\n\n\nA seed is an individual URL that the crawler uses as the starting point for the crawl. The seeds tab displays each seed that was included in that crawl, which allows you to see what sites you were targeting with that crawl.\n\n\n\n\nHosts\n\n\nThe hosts report shows every host (URL) that Archive-It attempted to crawl and capture. In addition to seed URLs, the crawler goes out and retrieves other content that is not stored directly on the site, but is included in the HTML code. This list will have a lot of unfamiliar URLs listed on it. The main focus of this tab should be on the number of queued URLs there are for each host. A queued URL is a URL that was identified by the crawler as a URL that should be crawled, but the crawler was unable to capture that URL due to time or data limits set for the crawl. If there are more than a handful, new scoping rules might be required.\n\n\n\n\nFile Types\n\n\nAllows you to see the types of files that were captured in your crawl. This can be helpful in finding out why a crawl might have exceeded the size that you thought it should be. It can also help you find out if individual documents that you are wishing to archive were in fact crawled.\n\n\n\n\nArchives\n\n\nThe archives page allows you to oversee and manage your active archive collections. It gives you an interface to search to find individual URLs within your archive. It essentially mimics the interface found on the public landing page http://archive-it.org/home/ncsu-libraries. You can search to see if something is in the archive, the crawl frequency that seed URL is set to as well as links to the landing page for that particular URL that shows each capture date for the seed URL.\n\n\n\n\n\n\n\n\nARS\n\n\nARS stands for Archive-It Research Services. From this page Archive-It customers can request a wide range of files for researchers. This is an additional paid service provided by Archive-It. Outside of the paid research filetypes, this page also includes a link to find out more information about downloading \nWARC\n files of your collection.\n\n\n\n\n\n\n\n\nThe additional, paid research file types are:\n* \nWANE\n: Web Archive Named Entities uses named-entity recognition tools to generate a list of all the people, places, and organizations mentioned in each URI in a collection along with a timestamp of URI capture.\n\n\n\n\n\n\nLGA\n: Longitudinal Graph Analysis files feature a complete list of what URIs link to what URIs, along with a timestamp, within an entire collection.\n\n\n\n\n\n\nWAT\n: Web Archive Transformation files feature key metadata elements that represent every crawled resource in a collection and are derived from a collection\u2019s WARC files.\n\n\n\n\n\n\nMore information on these filetypes can be found on the \nArchive-It support page\n.", 
            "title": "Admin Interface"
        }, 
        {
            "location": "/admin/#archive-it-admin-interface", 
            "text": "NCSU Libraries uses the Archive-It web archiving service to create our web archives. This service, provided by the  Internet Archive , allows us to manage the crawl, capture and storage of websites. Archive-It is a subscription service with an annual fee. To learn more about Archive-It and the Internet Archive you can visit  https://archive.org/about/ .", 
            "title": "Archive-It Admin Interface"
        }, 
        {
            "location": "/admin/#access-restrictions-user-roles", 
            "text": "The Archive-It administrator interface can be accessed by logging into  https://partner.archive-it.org/archiveit . In order to log in you need to be added as an administrator for our account. There are three levels of access that can be granted:   Administrator - Full administrative privileges. Can add additional users and manage all aspects of the account.  User - Can adjust all aspects of the collection, but are not administrators (can not add users)  Contributor - Lowest level of privileges. They can access the account and see crawl reports, but cannot add or remove seeds, start or stop crawls. This setting is good for staff members conducting QA.", 
            "title": "Access Restrictions / User Roles"
        }, 
        {
            "location": "/admin/#main-administrative-functions", 
            "text": "This is a brief overview of each of the areas available within the admin interface. For more in-depth documentation please visit the  Archive-It Help Center . The links below link to our Archive-It administration pages so a valid Archive-It login is required.", 
            "title": "Main Administrative Functions"
        }, 
        {
            "location": "/admin/#home", 
            "text": "When you first log in to the administrative interface you will see the home screen. This screen displays a list of your collections as well as a graph showing your current data usage. Currently we are allowed 1TB of data per subscription cycle (February - February). This page also displays data collection rates for each individual collection for the current subscription period. It is a good way to monitor the data usage, number of seed URLs and recent crawl dates for all of your collections.", 
            "title": "Home"
        }, 
        {
            "location": "/admin/#collections", 
            "text": "The collections page lists all collections within our account. We base our collections around our current collecting areas. Our primary collection is our NC State University Websites. From this page you can create a new collection, or monitor existing collections. The collections page shows current data, all time data and the number of active seeds for each collection. It is important to note that current data is data collected during the current one-year subscription.", 
            "title": "Collections"
        }, 
        {
            "location": "/admin/#crawls", 
            "text": "The crawls tab provides access to crawl reports, a list of current crawls, a list of scheduled crawls as well as a way to compare two crawls to one another. You can download lists of crawl reports or individual items from within a crawl report from this area. This area also provides a search box allowing you to search by Crawl ID, Status or Frequency.       Current Crawls :\nThe current crawls lists every crawl during the current subscription period for your account.    Test Crawls :\nThe test crawls tab filters all of the crawl reports to only show reports that pertain to test crawls. This allows you to narrow down the list to see only completed test crawls. These reports are useful when you begin to work on scoping new seeds, and run test crawls to test what the captured data will look like.    Scheduled Crawls \nThe scheduled crawls tab shows a list of each collection you have and what crawls are scheduled to run for those collections. It also notes whether or not you have set crawl data limits for any of those regularly scheduled crawls.", 
            "title": "Crawls"
        }, 
        {
            "location": "/admin/#crawl-reports", 
            "text": "The crawl reports tab shows a full list of all completed crawls. This includes completed regularly scheduled crawls as well as test and patch crawls. Click on the crawl ID to open an individual crawl report. Individual crawl reports show an overview of the data collected during that crawl. In addition to overall data numbers the following can be accessed from a crawl report:", 
            "title": "Crawl Reports"
        }, 
        {
            "location": "/admin/#seeds", 
            "text": "A seed is an individual URL that the crawler uses as the starting point for the crawl. The seeds tab displays each seed that was included in that crawl, which allows you to see what sites you were targeting with that crawl.", 
            "title": "Seeds"
        }, 
        {
            "location": "/admin/#hosts", 
            "text": "The hosts report shows every host (URL) that Archive-It attempted to crawl and capture. In addition to seed URLs, the crawler goes out and retrieves other content that is not stored directly on the site, but is included in the HTML code. This list will have a lot of unfamiliar URLs listed on it. The main focus of this tab should be on the number of queued URLs there are for each host. A queued URL is a URL that was identified by the crawler as a URL that should be crawled, but the crawler was unable to capture that URL due to time or data limits set for the crawl. If there are more than a handful, new scoping rules might be required.", 
            "title": "Hosts"
        }, 
        {
            "location": "/admin/#file-types", 
            "text": "Allows you to see the types of files that were captured in your crawl. This can be helpful in finding out why a crawl might have exceeded the size that you thought it should be. It can also help you find out if individual documents that you are wishing to archive were in fact crawled.", 
            "title": "File Types"
        }, 
        {
            "location": "/admin/#archives", 
            "text": "The archives page allows you to oversee and manage your active archive collections. It gives you an interface to search to find individual URLs within your archive. It essentially mimics the interface found on the public landing page http://archive-it.org/home/ncsu-libraries. You can search to see if something is in the archive, the crawl frequency that seed URL is set to as well as links to the landing page for that particular URL that shows each capture date for the seed URL.", 
            "title": "Archives"
        }, 
        {
            "location": "/admin/#ars", 
            "text": "ARS stands for Archive-It Research Services. From this page Archive-It customers can request a wide range of files for researchers. This is an additional paid service provided by Archive-It. Outside of the paid research filetypes, this page also includes a link to find out more information about downloading  WARC  files of your collection.     The additional, paid research file types are:\n*  WANE : Web Archive Named Entities uses named-entity recognition tools to generate a list of all the people, places, and organizations mentioned in each URI in a collection along with a timestamp of URI capture.    LGA : Longitudinal Graph Analysis files feature a complete list of what URIs link to what URIs, along with a timestamp, within an entire collection.    WAT : Web Archive Transformation files feature key metadata elements that represent every crawled resource in a collection and are derived from a collection\u2019s WARC files.    More information on these filetypes can be found on the  Archive-It support page .", 
            "title": "ARS"
        }, 
        {
            "location": "/support/", 
            "text": "Support Center\n\n\nThe \nArchive-It Help Center\n should be your first stop when looking into any technical issue that you experience while using the Archive-It service. It hosts a variety of tools, including user guide documentation, a community discussion board, and a support ticket system to submit issues for further review.\n\n\nUser Guide\n\n\nThe \nArchive-It User Guide\n is the primary documentation for the Archive-It service. Here you can find detailed documentation on every aspect of the Archive-It system. It includes 'Getting Started' documentation covering everything from setting up and administering your account, to more advanced topics related to seed management and ongoing maintenance of your account.\n\n\nCommunity Center\n\n\nThe \nArchive-It Community Center\n is a community based communications tool. It features posts by members of the Archive-It community. The posts relate to recent Archive-It announcements as well as discussions of best practices and practical tips that assist in resolving common issues.\n\n\nSupport Tickets\n\n\nIf you are unable to find the answer to your question in the User Guide or on the Community pages, you should submit a support ticket to the Archive-It staff through the \nSubmit a Request\n link in the upper right-hand side of the page.", 
            "title": "Support Center"
        }, 
        {
            "location": "/support/#support-center", 
            "text": "The  Archive-It Help Center  should be your first stop when looking into any technical issue that you experience while using the Archive-It service. It hosts a variety of tools, including user guide documentation, a community discussion board, and a support ticket system to submit issues for further review.", 
            "title": "Support Center"
        }, 
        {
            "location": "/support/#user-guide", 
            "text": "The  Archive-It User Guide  is the primary documentation for the Archive-It service. Here you can find detailed documentation on every aspect of the Archive-It system. It includes 'Getting Started' documentation covering everything from setting up and administering your account, to more advanced topics related to seed management and ongoing maintenance of your account.", 
            "title": "User Guide"
        }, 
        {
            "location": "/support/#community-center", 
            "text": "The  Archive-It Community Center  is a community based communications tool. It features posts by members of the Archive-It community. The posts relate to recent Archive-It announcements as well as discussions of best practices and practical tips that assist in resolving common issues.", 
            "title": "Community Center"
        }, 
        {
            "location": "/support/#support-tickets", 
            "text": "If you are unable to find the answer to your question in the User Guide or on the Community pages, you should submit a support ticket to the Archive-It staff through the  Submit a Request  link in the upper right-hand side of the page.", 
            "title": "Support Tickets"
        }, 
        {
            "location": "/seeds/", 
            "text": "Seed Management\n\n\nPerhaps the most important part of maintaining our web archive is seed management. Seed management is the act of selecting, scoping, scheduling, crawling and maintaining all of the seeds in our collections. While it is not intended to be an exhaustive list of subjects related to seed management, this page should guide you through the process of selecting a seed and prepping it for regular scheduled crawls and ongoing maintenance.\n\n\nSeed Management Workflow\n\n\n\n\nSeed Selection\n\n\nSeed selection is an ongoing process for all of our collections. Generally seeds are selected by curators. Suggested seeds are then added to our \nUp To Date Web Archiving Seeds Google Sheet\n (login required). This sheet allows us to track all seeds from one central location, and to track the seeds that have been proposed from various curators and library staff. Once a proposed seed is selected for archiving the process of seed scoping begins.\n\n\nAdding New Seeds to a Collection\n\n\nAfter a new seed has been selected, it is time to add that seed to a collection in Archive-It. In order to add a seed to a collection you need to:\n\n\n1) Open the Collection from the \nArchive-It homepage\n\n\n2) Click on the Seeds tab\n\n\n3) Click Add Seeds\n\n\n4) Paste the full URL of the seed you wish to add (e.g. https://www.ncsu.edu/)\n\n\n5) Set visibility to private\n\n\n6) Keep frequency as one time crawl (this ensures it does not accidentally end up in a recurring crawl before scoping is complete)\n\n\n7) Select \nseed type\n\n\n8) Click Add Seeds", 
            "title": "Selection and Addition"
        }, 
        {
            "location": "/seeds/#seed-management", 
            "text": "Perhaps the most important part of maintaining our web archive is seed management. Seed management is the act of selecting, scoping, scheduling, crawling and maintaining all of the seeds in our collections. While it is not intended to be an exhaustive list of subjects related to seed management, this page should guide you through the process of selecting a seed and prepping it for regular scheduled crawls and ongoing maintenance.", 
            "title": "Seed Management"
        }, 
        {
            "location": "/seeds/#seed-management-workflow", 
            "text": "", 
            "title": "Seed Management Workflow"
        }, 
        {
            "location": "/seeds/#seed-selection", 
            "text": "Seed selection is an ongoing process for all of our collections. Generally seeds are selected by curators. Suggested seeds are then added to our  Up To Date Web Archiving Seeds Google Sheet  (login required). This sheet allows us to track all seeds from one central location, and to track the seeds that have been proposed from various curators and library staff. Once a proposed seed is selected for archiving the process of seed scoping begins.", 
            "title": "Seed Selection"
        }, 
        {
            "location": "/seeds/#adding-new-seeds-to-a-collection", 
            "text": "After a new seed has been selected, it is time to add that seed to a collection in Archive-It. In order to add a seed to a collection you need to:  1) Open the Collection from the  Archive-It homepage  2) Click on the Seeds tab  3) Click Add Seeds  4) Paste the full URL of the seed you wish to add (e.g. https://www.ncsu.edu/)  5) Set visibility to private  6) Keep frequency as one time crawl (this ensures it does not accidentally end up in a recurring crawl before scoping is complete)  7) Select  seed type  8) Click Add Seeds", 
            "title": "Adding New Seeds to a Collection"
        }, 
        {
            "location": "/test-crawls/", 
            "text": "Test crawls\n\n\nRunning test crawls provides you with a mechanism to see how a crawl will behave with the current seed settings, without having to worry about the data captured during a test crawl being saved. It is a safe way to test changes to seeds or collections that will highlight any scoping errors that may cause problems down the road. The data from a test crawl can either be saved (if the crawl completed as expected) or deleted, if there were errors in the crawl that need to be addressed. The first step in properly scoping any seed to run a test crawl.\n\n\nTest Crawl Individual Seeds\n\n\nIn order to see how the addition of an individual new seed to a collection will affect that collection, you should start by running a test crawl on that seed. To run a test crawl on an individual seed you should:\n\n\n1) Open the Collection Overview page for the collection that you have added the seed to.\n\n\n2) Check the checkbox next to the seed you wish to run a test crawl on.\n\n\n3) Click on 'Run Crawl'\n\n\n4) Select 'Test Crawl' as crawl type\n\n\n5) Set the duration equal to the duration of the production crawl the seed will be added to. For example if the seed will be in a collection with a recurring monthly crawl that lasts three days, set the time limit to three days.\n\n\n6) Click on Crawl\n\n\nTest Crawl a Batch of Seeds\n\n\nArchive-It limits the amount of concurrent test crawls that can run at any given point in time. If you are adding multiple seeds to a collection, that will be crawled with identical frequency and duration, you can run a test crawl that includes multiple seeds. To do so you would follow the same steps as above, but just select the checkbox of every seed that you wish to include in the test crawl.\n\n\nTest Crawl Reports\n\n\nTest crawl reports are identical to production crawl reports. They are intended to give you a complete overview of the data captured during a crawl as well as links to the wayback captures of the site for quality assurance review. Test crawl reports can be found by going to the main 'Crawls' tab of the Archive-It admin interface and then clicking on 'Test Crawls'. There are a few things to pay particularly close attention to when evaluating the effectiveness of a test crawl.\n\n\n\n\n\n\nCapture Completeness - This is best evaluated by looking at the wayback capture of the page and comparing it to the 'live' site. You want to ensure that all items on the live site were included on the wayback capture. You can look at layout, photos, navigation items and videos.\n\n\n\n\n\n\nData Budget - The overview page of the crawl report shows you how much data was gathered during the crawl. You can compare this to other crawls you have completed to see if it appears to be an appropriate amount of data.\n\n\n\n\n\n\nQueued URLs - When looking at the test crawl report, you can examine the hosts list for each seed URL that was included in the test crawl. When examining the hosts list pay attention to the number of 'Queued' URLs on the report. This number (and the URL list generated by clicking on the number) shows you URLs that the crawler detected, and wanted to crawl, but did not have time to crawl before the limit was reached. Ideally this number would be 0.\n\n\n\n\nFixing Queued URLs - You can remedy a large Queued URL number by increasing the crawl duration or by modifying scoping rules to ensure that any crawler traps are avoided. An example of a crawler trap would be a calendar where every day is a link even if there is no event or page associated with that link which could create thousands of 'false' links. The crawler sees each link as valid and will try to follow each one for as far into the future as the calendar is programed to show. More information on crawler traps can be found \nhere\n. \n\n\n\n\n\n\n\n\nConvert Test Crawl to Production Crawl\n\n\nOnce you have finished the QA on a test crawl and are confident that it is ready to be added as a production seed, do the following:\n\n\n1) Open the collection landing page that contains the seed\n\n\n2) Click on the 'Seeds' tab\n\n\n3) Click the checkbox next to the seed you wish to add to a production crawl\n\n\n4) Click 'Edit Settings'\n\n\n5) Check the checkbox labeled 'Visible to the public'\n\n\n6) Set the crawl frequency\n\n\n7) Click 'Save'", 
            "title": "Test Crawls"
        }, 
        {
            "location": "/test-crawls/#test-crawls", 
            "text": "Running test crawls provides you with a mechanism to see how a crawl will behave with the current seed settings, without having to worry about the data captured during a test crawl being saved. It is a safe way to test changes to seeds or collections that will highlight any scoping errors that may cause problems down the road. The data from a test crawl can either be saved (if the crawl completed as expected) or deleted, if there were errors in the crawl that need to be addressed. The first step in properly scoping any seed to run a test crawl.", 
            "title": "Test crawls"
        }, 
        {
            "location": "/test-crawls/#test-crawl-individual-seeds", 
            "text": "In order to see how the addition of an individual new seed to a collection will affect that collection, you should start by running a test crawl on that seed. To run a test crawl on an individual seed you should:  1) Open the Collection Overview page for the collection that you have added the seed to.  2) Check the checkbox next to the seed you wish to run a test crawl on.  3) Click on 'Run Crawl'  4) Select 'Test Crawl' as crawl type  5) Set the duration equal to the duration of the production crawl the seed will be added to. For example if the seed will be in a collection with a recurring monthly crawl that lasts three days, set the time limit to three days.  6) Click on Crawl", 
            "title": "Test Crawl Individual Seeds"
        }, 
        {
            "location": "/test-crawls/#test-crawl-a-batch-of-seeds", 
            "text": "Archive-It limits the amount of concurrent test crawls that can run at any given point in time. If you are adding multiple seeds to a collection, that will be crawled with identical frequency and duration, you can run a test crawl that includes multiple seeds. To do so you would follow the same steps as above, but just select the checkbox of every seed that you wish to include in the test crawl.", 
            "title": "Test Crawl a Batch of Seeds"
        }, 
        {
            "location": "/test-crawls/#test-crawl-reports", 
            "text": "Test crawl reports are identical to production crawl reports. They are intended to give you a complete overview of the data captured during a crawl as well as links to the wayback captures of the site for quality assurance review. Test crawl reports can be found by going to the main 'Crawls' tab of the Archive-It admin interface and then clicking on 'Test Crawls'. There are a few things to pay particularly close attention to when evaluating the effectiveness of a test crawl.    Capture Completeness - This is best evaluated by looking at the wayback capture of the page and comparing it to the 'live' site. You want to ensure that all items on the live site were included on the wayback capture. You can look at layout, photos, navigation items and videos.    Data Budget - The overview page of the crawl report shows you how much data was gathered during the crawl. You can compare this to other crawls you have completed to see if it appears to be an appropriate amount of data.    Queued URLs - When looking at the test crawl report, you can examine the hosts list for each seed URL that was included in the test crawl. When examining the hosts list pay attention to the number of 'Queued' URLs on the report. This number (and the URL list generated by clicking on the number) shows you URLs that the crawler detected, and wanted to crawl, but did not have time to crawl before the limit was reached. Ideally this number would be 0.   Fixing Queued URLs - You can remedy a large Queued URL number by increasing the crawl duration or by modifying scoping rules to ensure that any crawler traps are avoided. An example of a crawler trap would be a calendar where every day is a link even if there is no event or page associated with that link which could create thousands of 'false' links. The crawler sees each link as valid and will try to follow each one for as far into the future as the calendar is programed to show. More information on crawler traps can be found  here .", 
            "title": "Test Crawl Reports"
        }, 
        {
            "location": "/test-crawls/#convert-test-crawl-to-production-crawl", 
            "text": "Once you have finished the QA on a test crawl and are confident that it is ready to be added as a production seed, do the following:  1) Open the collection landing page that contains the seed  2) Click on the 'Seeds' tab  3) Click the checkbox next to the seed you wish to add to a production crawl  4) Click 'Edit Settings'  5) Check the checkbox labeled 'Visible to the public'  6) Set the crawl frequency  7) Click 'Save'", 
            "title": "Convert Test Crawl to Production Crawl"
        }, 
        {
            "location": "/scoping/", 
            "text": "Seed Scoping\n\n\nSeed scoping is the process of setting parameters that determine how much or how little of a site the Archive-It crawler reaches. This process is used to ensure that you are collecting all of the pages you want to collect, and avoiding those which you do not wish to become part of the collection. Seed scoping is handled through a variety of tools within Archive-It. Seed scoping is perhaps the most important element in web archiving. Proper scoping allows you to be very selective with the web content that you wish to add to your archive. It also assists you in reducing duplication and preserving your data budget.\n\n\nSeed Type\n\n\nProperly setting the \nseed type\n is the first step in seed scoping. There are a variety of seed types available\n\n\n\n\n\n\nStandard\n: This follows the '\ndefault seed scoping\n' and should be used for most sites. It will follow links within the site  and capture embedded content within the site. If scoped properly it will not capture content outside of the seed site.\n\n\n\n\n\n\nStandard+\n: This will capture everything that would be captured in the standard crawl, plus external links.\n\n\n\n\n\n\nOne Page\n: This will only capture the first page of your seed. This scope setting is great for capturing one page articles, like those included in the \nJames B. Hunt Jr. Library Impact Collection\n.\n\n\n\n\n\n\nOne Page+\n: This will capture the first page of your seed, as well as the first page of any URLs that are linked off of that seed.\n\n\n\n\n\n\nScoping Rules\n\n\nThere are a variety of scoping rules available that allow you to either expand or contract the scope of your crawl. Below you will find a brief description of each. There is also thorough documentation about crawl scope on the Archive-It support pages.\n\n\n\n\n\n\nBlock Hosts\n - A host is a URL that is crawled as part of your seed URL in order to deliver embedded content. If you find that one particular host is problematic, you can block it by adding a 'block hosts' scoping rule for that particluar host.\n\n\n\n\n\n\nAdd Data Limits\n - Certain seed URLs may be quite data heavy particularly if they involve a large number of videos or images. If you wish to limit the amount of data collected for any seed you can set a data limit. Data limits are a good way to ensure that recurring crawls don't exceed a certain size.\n\n\n\n\n\n\nIgnore Robots.txt\n - The robots.txt file that is embedded in many websites is a technical file that outlines which, if any, robots are allowed to crawl that site. If it excludes all robots then the Archive-It crawler will not be able to crawl the website. The best practice is to contact the site owner and have them \nadd an exclusion for the Archive-It crawler\n. However, in some cases such as when crawling content that is owned by your institution, you may wish to add an exclude robots.txt scoping rule to ensure all content is crawled.\n\n\n\n\n\n\n Block URL if...\n - You may wish to block certain URLs from your crawl based on a portion of the URL rather than having to set up a large number of specific hosts to block. This comes in handy when URLs that should be out of scope either all contain the same text, or can be matched using some sort of regular expression. The use of these two tools can be powerful in allowing you to avoid crawler traps, or crawling unwanted content.\n\n\n\n\n\n\nContains the Text\n - If each URL you wish to block contains a specific string of text that does not appear in the URLs you wish to crawl, you can enter that here to block every URL containing that string of text. For example, if you wish to block every URL for the ncsu.edu seed that contains the string 'ncsu.edu/private/' you could add /private/ to the field.\n\n\n\n\n\n\nMatches the Regular Expression\n - A \nregular expression\n is a way to look for common patterns in the url and block based on pattern matches, instead of straight text maches. For example you might have a url patter that is url.com/09-09.html and url.com/10-10.html both of which you want to block. Rather than having to enter a 'contains the text' rule for every single match you can use a regular expression to find all of the matches. A common use of 'Block URL if it Matches the Regular Expression' regular scoping rule is to help the crawler avoid crawler traps. A common crawler trap is a calendar on a website in which every date on the calendar is a URL. To the crawler each link looks valid, even though the calendar is dynamically generating a never ending set of URLs. View more about the use of regular expressions in Archive-It by viewing this \nsupport document\n. \n\n\n\n\n\n\n\n\n\n\nExpand Scope if...\n\nThis string is similar to the Block URL If... setting, but is used to the opposite effect. If there is a string of text common to URLs that you wish to crawl, you can add that here either as a string or as a regular expression.\n\n\n\n\n\n\nCollection Level Scoping Rules\n\n\nCollection level scoping rules can be set from the Crawl Scope tab on the landing page of any collection. Collection level scoping rules apply to each seed in that collection. Collection level scoping rules are best utilized for content that may be consistent across seeds in your collection that you are sure you want to capture. This level of scoping can be useful to ensure you set the \nproper scoping rules\n to capture all YouTube videos for each seed in the collection.\n\n\nSeed Level Scoping Rules\n\n\nMore often than not, you will be setting seed level scoping rules rather than collection level scoping rules. These rules are set by opening up the settings pane for any seed in your collection, and clicking on Seed Scope. You use the same scope contraction and expansion rules as outlined above. However, rather than those rules being applied to every seed within your collection, they are applied to individual seeds.", 
            "title": "Scoping"
        }, 
        {
            "location": "/scoping/#seed-scoping", 
            "text": "Seed scoping is the process of setting parameters that determine how much or how little of a site the Archive-It crawler reaches. This process is used to ensure that you are collecting all of the pages you want to collect, and avoiding those which you do not wish to become part of the collection. Seed scoping is handled through a variety of tools within Archive-It. Seed scoping is perhaps the most important element in web archiving. Proper scoping allows you to be very selective with the web content that you wish to add to your archive. It also assists you in reducing duplication and preserving your data budget.", 
            "title": "Seed Scoping"
        }, 
        {
            "location": "/scoping/#seed-type", 
            "text": "Properly setting the  seed type  is the first step in seed scoping. There are a variety of seed types available    Standard : This follows the ' default seed scoping ' and should be used for most sites. It will follow links within the site  and capture embedded content within the site. If scoped properly it will not capture content outside of the seed site.    Standard+ : This will capture everything that would be captured in the standard crawl, plus external links.    One Page : This will only capture the first page of your seed. This scope setting is great for capturing one page articles, like those included in the  James B. Hunt Jr. Library Impact Collection .    One Page+ : This will capture the first page of your seed, as well as the first page of any URLs that are linked off of that seed.", 
            "title": "Seed Type"
        }, 
        {
            "location": "/scoping/#scoping-rules", 
            "text": "There are a variety of scoping rules available that allow you to either expand or contract the scope of your crawl. Below you will find a brief description of each. There is also thorough documentation about crawl scope on the Archive-It support pages.    Block Hosts  - A host is a URL that is crawled as part of your seed URL in order to deliver embedded content. If you find that one particular host is problematic, you can block it by adding a 'block hosts' scoping rule for that particluar host.    Add Data Limits  - Certain seed URLs may be quite data heavy particularly if they involve a large number of videos or images. If you wish to limit the amount of data collected for any seed you can set a data limit. Data limits are a good way to ensure that recurring crawls don't exceed a certain size.    Ignore Robots.txt  - The robots.txt file that is embedded in many websites is a technical file that outlines which, if any, robots are allowed to crawl that site. If it excludes all robots then the Archive-It crawler will not be able to crawl the website. The best practice is to contact the site owner and have them  add an exclusion for the Archive-It crawler . However, in some cases such as when crawling content that is owned by your institution, you may wish to add an exclude robots.txt scoping rule to ensure all content is crawled.     Block URL if...  - You may wish to block certain URLs from your crawl based on a portion of the URL rather than having to set up a large number of specific hosts to block. This comes in handy when URLs that should be out of scope either all contain the same text, or can be matched using some sort of regular expression. The use of these two tools can be powerful in allowing you to avoid crawler traps, or crawling unwanted content.    Contains the Text  - If each URL you wish to block contains a specific string of text that does not appear in the URLs you wish to crawl, you can enter that here to block every URL containing that string of text. For example, if you wish to block every URL for the ncsu.edu seed that contains the string 'ncsu.edu/private/' you could add /private/ to the field.    Matches the Regular Expression  - A  regular expression  is a way to look for common patterns in the url and block based on pattern matches, instead of straight text maches. For example you might have a url patter that is url.com/09-09.html and url.com/10-10.html both of which you want to block. Rather than having to enter a 'contains the text' rule for every single match you can use a regular expression to find all of the matches. A common use of 'Block URL if it Matches the Regular Expression' regular scoping rule is to help the crawler avoid crawler traps. A common crawler trap is a calendar on a website in which every date on the calendar is a URL. To the crawler each link looks valid, even though the calendar is dynamically generating a never ending set of URLs. View more about the use of regular expressions in Archive-It by viewing this  support document .       Expand Scope if... \nThis string is similar to the Block URL If... setting, but is used to the opposite effect. If there is a string of text common to URLs that you wish to crawl, you can add that here either as a string or as a regular expression.", 
            "title": "Scoping Rules"
        }, 
        {
            "location": "/scoping/#collection-level-scoping-rules", 
            "text": "Collection level scoping rules can be set from the Crawl Scope tab on the landing page of any collection. Collection level scoping rules apply to each seed in that collection. Collection level scoping rules are best utilized for content that may be consistent across seeds in your collection that you are sure you want to capture. This level of scoping can be useful to ensure you set the  proper scoping rules  to capture all YouTube videos for each seed in the collection.", 
            "title": "Collection Level Scoping Rules"
        }, 
        {
            "location": "/scoping/#seed-level-scoping-rules", 
            "text": "More often than not, you will be setting seed level scoping rules rather than collection level scoping rules. These rules are set by opening up the settings pane for any seed in your collection, and clicking on Seed Scope. You use the same scope contraction and expansion rules as outlined above. However, rather than those rules being applied to every seed within your collection, they are applied to individual seeds.", 
            "title": "Seed Level Scoping Rules"
        }, 
        {
            "location": "/maintenance/", 
            "text": "Ongoing Maintenance\n\n\nIn order to maintain consistency throughout the archive, and to ensure that it is properly updated there are a few recurring tasks that should be regularly carried out. By regularly updating crawl schedules, seed redirections and the active state of seeds in the collection we can ensure that the collection we are presenting aligns well with our collecting strategy.\n\n\nCrawl Schedules\n\n\nPart of the organizational structure of our web archive is based on grouping similar sites together, and crawling them at similar frequencies. This allows us to crawl news sites with one frequency, major college or university-wide departmental sites with another, and smaller sites with yet another frequency. These frequencies roughly correspond to the likelihood that information on that website will change. You will also notice a correlation between the number of seeds and the frequency of crawls, the more frequent the crawl the fewer seeds that it is likely to contain. Currently we have the following regular crawls scheduled:\n\n\n\n\n\n\nDaily: The websites in our daily crawls include the main university website (ncsu.edu), the NC State University News site (news.ncsu.edu) as well as the two campus newspapers. Information on these sites is likely to change on a daily basis, so it is important to capture that data while it is available.\n\n\n\n\n\n\nWeekly: The websites included in our weekly crawls include the college and department level news sites. These sites change frequently, but not frequently enough to require daily crawling.\n\n\n\n\n\n\nMonthly: The websites in our monthly crawls mostly represent the main college-level websites as well as the websites of campus wide administrative units such as the libraries of the office of information technology.\n\n\n\n\n\n\nQuarterly: The websites in our quarterly crawls represent smaller department level websites throughout the organization.\n\n\n\n\n\n\nURL Redirection / Seed Updating\n\n\nFrom time to time website configurations may change and a particular URL may be swapped out for a new version. Generally when this happens the old URL is redirected to the new URL for some period of time. However, redirections are rarely permanent. In the scope of Archive-It there are two types of redirections that we should be aware of, both should be handled differently.\n\n\n\n\n\n\nHTTP vs HTTPS - Often when adding a seed we may not notice that the site subtly redirects to an HTTPS version of the website even if the seed specified HTTP. It is best to catch these redirections while conducting QA before placing the seed into a scheduled crawl. However, if regular crawls have started, and you don't want a new seed it is likely fine as long as the seed URL is the insecure (HTTP) and is redirecting to the secure (HTTPS) version. The reason it is okay to rely on this redirection to continue is that the domain hasn't really changed, so the issue of the old domain going away (no longer being valid and registered) should not exist.\n\n\n\n\n\n\nPermanent - Due to regular updates of websites the URL structure may change. For example https://ncsu.edu/engr may become https://engr.ncsu.edu. This would be an example of a permanent, deliberate URL redirection. In this case you would likely want to add the new URL as a new seed, and use seed metadata to note where the new URL is being redirected from and where the old URL is redirecting to. You will also want to mark the old URL as inactive in the seed settings panel.\n\n\n\n\n\n\nActive / Inactive Seed and Collection Management\n\n\nArchive-It gives administrators the option to label seeds or collections as active or inactive. Active seeds are crawled according to the schedule they are placed on. Inactive seeds remain visible as part of the collection but are not crawled according to the crawl schedule. The most common reason for setting a seed as inactive is that the URL for the seed is no longer valid. Either the website was taken down or the URL has been permanently redirected to a new URL. By marking it as inactive, visitors to the collection can still find the seed, and see all of the capture dates.", 
            "title": "Maintenance"
        }, 
        {
            "location": "/maintenance/#ongoing-maintenance", 
            "text": "In order to maintain consistency throughout the archive, and to ensure that it is properly updated there are a few recurring tasks that should be regularly carried out. By regularly updating crawl schedules, seed redirections and the active state of seeds in the collection we can ensure that the collection we are presenting aligns well with our collecting strategy.", 
            "title": "Ongoing Maintenance"
        }, 
        {
            "location": "/maintenance/#crawl-schedules", 
            "text": "Part of the organizational structure of our web archive is based on grouping similar sites together, and crawling them at similar frequencies. This allows us to crawl news sites with one frequency, major college or university-wide departmental sites with another, and smaller sites with yet another frequency. These frequencies roughly correspond to the likelihood that information on that website will change. You will also notice a correlation between the number of seeds and the frequency of crawls, the more frequent the crawl the fewer seeds that it is likely to contain. Currently we have the following regular crawls scheduled:    Daily: The websites in our daily crawls include the main university website (ncsu.edu), the NC State University News site (news.ncsu.edu) as well as the two campus newspapers. Information on these sites is likely to change on a daily basis, so it is important to capture that data while it is available.    Weekly: The websites included in our weekly crawls include the college and department level news sites. These sites change frequently, but not frequently enough to require daily crawling.    Monthly: The websites in our monthly crawls mostly represent the main college-level websites as well as the websites of campus wide administrative units such as the libraries of the office of information technology.    Quarterly: The websites in our quarterly crawls represent smaller department level websites throughout the organization.", 
            "title": "Crawl Schedules"
        }, 
        {
            "location": "/maintenance/#url-redirection-seed-updating", 
            "text": "From time to time website configurations may change and a particular URL may be swapped out for a new version. Generally when this happens the old URL is redirected to the new URL for some period of time. However, redirections are rarely permanent. In the scope of Archive-It there are two types of redirections that we should be aware of, both should be handled differently.    HTTP vs HTTPS - Often when adding a seed we may not notice that the site subtly redirects to an HTTPS version of the website even if the seed specified HTTP. It is best to catch these redirections while conducting QA before placing the seed into a scheduled crawl. However, if regular crawls have started, and you don't want a new seed it is likely fine as long as the seed URL is the insecure (HTTP) and is redirecting to the secure (HTTPS) version. The reason it is okay to rely on this redirection to continue is that the domain hasn't really changed, so the issue of the old domain going away (no longer being valid and registered) should not exist.    Permanent - Due to regular updates of websites the URL structure may change. For example https://ncsu.edu/engr may become https://engr.ncsu.edu. This would be an example of a permanent, deliberate URL redirection. In this case you would likely want to add the new URL as a new seed, and use seed metadata to note where the new URL is being redirected from and where the old URL is redirecting to. You will also want to mark the old URL as inactive in the seed settings panel.", 
            "title": "URL Redirection / Seed Updating"
        }, 
        {
            "location": "/maintenance/#active-inactive-seed-and-collection-management", 
            "text": "Archive-It gives administrators the option to label seeds or collections as active or inactive. Active seeds are crawled according to the schedule they are placed on. Inactive seeds remain visible as part of the collection but are not crawled according to the crawl schedule. The most common reason for setting a seed as inactive is that the URL for the seed is no longer valid. Either the website was taken down or the URL has been permanently redirected to a new URL. By marking it as inactive, visitors to the collection can still find the seed, and see all of the capture dates.", 
            "title": "Active / Inactive Seed and Collection Management"
        }, 
        {
            "location": "/metadata/", 
            "text": "Seed Metadata\n\n\nEach seed in the collection should be assigned metadata as it is added to the collection. There are default metadata fields provided by Archive-It. More information on how Archive-It manages metadata and their recommendations for metadata management can be found at \nhttps://support.archive-it.org/hc/en-us/articles/208332603-Add-edit-and-manage-your-metadata\n.\n\n\nIn addition to the default metadata fields provided by Archive-It, you can also create custom metadata fields. In our NC State University Websites Collection we have created custom metadata fields to record when a seed url is being redirected to from an old url or when an old seed url is redirecting to a new seed url. Currently, all metadata that is added to seeds is publicly viewable from the collection and seed level views found at \nhttps://archive-it.org/home/ncsu-libraries\n.\n\n\nThe follow is an example of the metadata fields that we are using for our \nNC State University Websites Collection\n. Fields will need to be added and removed as found appropriate for other collections.\n\n\n\n\nURL - The full and valid URL for the site as it is represented in Archive-It  \n\n\nLanguage - The primary language of the website.\n\n\nCreator - The creator of the website.\n\n\nCollector - 'NCSU Libraries'\n\n\nTitle - The website title as it appears in the title meta tag in the website source.\n\n\nPublisher - The creator of the website.\n\n\nDescription - A full description of the website.\n\n\nRedirecting To - The URL that the seed is now redirecting to, if the redirection is permanent.\n\n\nRedirecting From - The URL that redirected to this seed, if the redirect is permanent.\n\n\nFormat - 'Web'\n\n\nSubject - The primary subject of the website.\n\n\n\n\n\n\nUpdating metadata\n\n\nAdding or Updating Individual Seed Metadata\n\n\nThe ability to create metadata for a seed happens only after the seed url is added to the collection (\nSee Adding new seeds to a Collection\n).\n\n\nYou can complete individual seed metadata updates by following these steps:\n\n\n1) Open the Archive-It admin panel\n\n\n2) Click on the collection that contains the seed you wish to add metadata for\n\n\n3) Find the individual seed you wish to update and click on that url to open the seed settings panel\n\n\n4) Click on the Metadata tab\n\n\n5) Click Edit\n\n\n6) Fill out each metadata field with the correct metadata, click 'add' after filling out each individual\nfield\n\n\n7) Once all of the fields have been updated, click 'done'\n\n\nBulk Metadata Updates\n\n\nArchive-It allows you to upload bulk metadata for all of your seeds. This is very useful if you have recently added a lot of new seeds to a collection, or if there is a new metadata field that you wish to add to all of the seeds in an existing collection. To learn more about bulk seed metadata importing and exporting can be found at \nhttps://support.archive-it.org/hc/en-us/articles/208012996-Upload-and-download-metadata\n.\n\n\nIt is important to note the difference between adding to existing metadata values and overwriting current metadata. You would use the 'add to existing metadata values' option if you were adding a new metadata field to every seed in your collection. You would use the 'overwrite existing seed metadata values' option if you were updating multiple seeds with metadata for the first time or if you had edited metadata extensively from an export (cleaning up exiting metadata).\n\n\nSeeds must exist in Archive-It before they will appear in the exported sheet. That means you must first \nadd all new seed urls\n that you wish to create metadata for before exporting the .ods file that you will add the metadata to.\n\n\nYou can complete a bulk metadata updates by following these steps:\n\n\n1) Navigate to the collection overview page for the collection that you added the new seeds to.\n\n\n2) Click on the Metadata tab\n\n\n3) Click on the Bulk Seed Metadata tab\n\n\n4) Click Download Existing Seed Metadata, make a duplicate of this file and store it somewhere safe. The file has an .ods extension. It can be opened by Microsoft Excel in Windows. If you are using a Mac it must be opened using \nOpenOffice Spreadsheets\n\n\n5) Add content to the metadata fields for each seed that you have added or wish to edit the metadata for\n\n\n6) Return to the Bulk Seed Metadata tab in Archive-It\n\n\n7) Export a second (backup) copy of the existing metadata, label the file with the current date and mark it clearly as backup and put it into Google Drive.\n\n\n7) Select 'Overwrite Existing Seed Metadata' and click upload\n\n\n8) Upload the .ods file that you modified with new metadata for the new seeds\n\n\n9) Confirm you wish to overwrite the existing metadata and review the preview to ensure the new metadata reflects your changes\n\n\n10) Save the new changes", 
            "title": "Metadata"
        }, 
        {
            "location": "/metadata/#seed-metadata", 
            "text": "Each seed in the collection should be assigned metadata as it is added to the collection. There are default metadata fields provided by Archive-It. More information on how Archive-It manages metadata and their recommendations for metadata management can be found at  https://support.archive-it.org/hc/en-us/articles/208332603-Add-edit-and-manage-your-metadata .  In addition to the default metadata fields provided by Archive-It, you can also create custom metadata fields. In our NC State University Websites Collection we have created custom metadata fields to record when a seed url is being redirected to from an old url or when an old seed url is redirecting to a new seed url. Currently, all metadata that is added to seeds is publicly viewable from the collection and seed level views found at  https://archive-it.org/home/ncsu-libraries .  The follow is an example of the metadata fields that we are using for our  NC State University Websites Collection . Fields will need to be added and removed as found appropriate for other collections.   URL - The full and valid URL for the site as it is represented in Archive-It    Language - The primary language of the website.  Creator - The creator of the website.  Collector - 'NCSU Libraries'  Title - The website title as it appears in the title meta tag in the website source.  Publisher - The creator of the website.  Description - A full description of the website.  Redirecting To - The URL that the seed is now redirecting to, if the redirection is permanent.  Redirecting From - The URL that redirected to this seed, if the redirect is permanent.  Format - 'Web'  Subject - The primary subject of the website.", 
            "title": "Seed Metadata"
        }, 
        {
            "location": "/metadata/#updating-metadata", 
            "text": "", 
            "title": "Updating metadata"
        }, 
        {
            "location": "/metadata/#adding-or-updating-individual-seed-metadata", 
            "text": "The ability to create metadata for a seed happens only after the seed url is added to the collection ( See Adding new seeds to a Collection ).  You can complete individual seed metadata updates by following these steps:  1) Open the Archive-It admin panel  2) Click on the collection that contains the seed you wish to add metadata for  3) Find the individual seed you wish to update and click on that url to open the seed settings panel  4) Click on the Metadata tab  5) Click Edit  6) Fill out each metadata field with the correct metadata, click 'add' after filling out each individual\nfield  7) Once all of the fields have been updated, click 'done'", 
            "title": "Adding or Updating Individual Seed Metadata"
        }, 
        {
            "location": "/metadata/#bulk-metadata-updates", 
            "text": "Archive-It allows you to upload bulk metadata for all of your seeds. This is very useful if you have recently added a lot of new seeds to a collection, or if there is a new metadata field that you wish to add to all of the seeds in an existing collection. To learn more about bulk seed metadata importing and exporting can be found at  https://support.archive-it.org/hc/en-us/articles/208012996-Upload-and-download-metadata .  It is important to note the difference between adding to existing metadata values and overwriting current metadata. You would use the 'add to existing metadata values' option if you were adding a new metadata field to every seed in your collection. You would use the 'overwrite existing seed metadata values' option if you were updating multiple seeds with metadata for the first time or if you had edited metadata extensively from an export (cleaning up exiting metadata).  Seeds must exist in Archive-It before they will appear in the exported sheet. That means you must first  add all new seed urls  that you wish to create metadata for before exporting the .ods file that you will add the metadata to.  You can complete a bulk metadata updates by following these steps:  1) Navigate to the collection overview page for the collection that you added the new seeds to.  2) Click on the Metadata tab  3) Click on the Bulk Seed Metadata tab  4) Click Download Existing Seed Metadata, make a duplicate of this file and store it somewhere safe. The file has an .ods extension. It can be opened by Microsoft Excel in Windows. If you are using a Mac it must be opened using  OpenOffice Spreadsheets  5) Add content to the metadata fields for each seed that you have added or wish to edit the metadata for  6) Return to the Bulk Seed Metadata tab in Archive-It  7) Export a second (backup) copy of the existing metadata, label the file with the current date and mark it clearly as backup and put it into Google Drive.  7) Select 'Overwrite Existing Seed Metadata' and click upload  8) Upload the .ods file that you modified with new metadata for the new seeds  9) Confirm you wish to overwrite the existing metadata and review the preview to ensure the new metadata reflects your changes  10) Save the new changes", 
            "title": "Bulk Metadata Updates"
        }, 
        {
            "location": "/new-seed-qa/", 
            "text": "New Seed QA\n\n\nEvery time you add a new seed to the archive it must undergo full quality assurance to ensure the capture of the website is complete. The general workflow for quality assurance of a new site is outlined below:\n\n\n\nGoogle QA Form\n\n\nFor each collection in Archive-It we have created an affiliated Google Form to assist in recording the results of the quality assurance tasks relevant to that collection. This form documents QA findings and creates Trello cards that allow us to track and manage any QA issues that arise from the crawl. An example form can be found \nhere\n.\n\n\nQualitative QA Analysis\n\n\nThe purpose of qualitative QA analysis is to ensure that the captured site renders similarly to the 'live' site it was meant to capture. In order to perform qualitative QA analysis on any captured site you need to open up the 'live' site as well as the wayback capture of the site (linked to from the crawl report). You want to compare the following:\n\n\n\n\nImages\n\n\nNavigation Items\n\n\nLayout\n\n\nVideo\n\n\nContent\n\n\n\n\nIt is possible, on quickly changing sites, that images and content on the archived site may be different from what you see on the live site. You will still want to make sure that images and content appear to be working correctly on the archived site.\n\n\nQuantitative QA Analysis\n\n\nThe crawl report contains a lot of data related to the crawl. This data can be used to help determine if a crawl was effective or not. When looking at the crawl report pay attention to these indicators:\n\n\n\n\nNew Data - This is the amount of new data collected during the crawl, this amount of data will go towards your total subscription data budget.\n\n\nHosts - This lists the individual hosts where content for the page was pulled from, it is helpful in determining if certain content is blocked, what needs to be added as a scope expansion rule.\n\n\nQueued URLs - This is the number of URLs that the crawler detected should be crawled, but were not crawled due to crawl limits. Often if this is a high number it is the result of a crawler trap.\n\n\nBlocked URLs - URLs that are blocked from the crawler, either by scoping rules or by a robot.txt file", 
            "title": "New Seed QA"
        }, 
        {
            "location": "/new-seed-qa/#new-seed-qa", 
            "text": "Every time you add a new seed to the archive it must undergo full quality assurance to ensure the capture of the website is complete. The general workflow for quality assurance of a new site is outlined below:", 
            "title": "New Seed QA"
        }, 
        {
            "location": "/new-seed-qa/#google-qa-form", 
            "text": "For each collection in Archive-It we have created an affiliated Google Form to assist in recording the results of the quality assurance tasks relevant to that collection. This form documents QA findings and creates Trello cards that allow us to track and manage any QA issues that arise from the crawl. An example form can be found  here .", 
            "title": "Google QA Form"
        }, 
        {
            "location": "/new-seed-qa/#qualitative-qa-analysis", 
            "text": "The purpose of qualitative QA analysis is to ensure that the captured site renders similarly to the 'live' site it was meant to capture. In order to perform qualitative QA analysis on any captured site you need to open up the 'live' site as well as the wayback capture of the site (linked to from the crawl report). You want to compare the following:   Images  Navigation Items  Layout  Video  Content   It is possible, on quickly changing sites, that images and content on the archived site may be different from what you see on the live site. You will still want to make sure that images and content appear to be working correctly on the archived site.", 
            "title": "Qualitative QA Analysis"
        }, 
        {
            "location": "/new-seed-qa/#quantitative-qa-analysis", 
            "text": "The crawl report contains a lot of data related to the crawl. This data can be used to help determine if a crawl was effective or not. When looking at the crawl report pay attention to these indicators:   New Data - This is the amount of new data collected during the crawl, this amount of data will go towards your total subscription data budget.  Hosts - This lists the individual hosts where content for the page was pulled from, it is helpful in determining if certain content is blocked, what needs to be added as a scope expansion rule.  Queued URLs - This is the number of URLs that the crawler detected should be crawled, but were not crawled due to crawl limits. Often if this is a high number it is the result of a crawler trap.  Blocked URLs - URLs that are blocked from the crawler, either by scoping rules or by a robot.txt file", 
            "title": "Quantitative QA Analysis"
        }, 
        {
            "location": "/recurring-crawl-qa/", 
            "text": "Recurring Crawl Quality Assurance\n\n\nWhile most of the heavy quality assurance work occurs during the process of adding a new seed to the collection, certain steps must take place to ensure that recurring crawls are not encountering any errors. There are two main aspects to recurring crawl quality assurance. During the qualitative analysis phase you will want to visually inspect websites as they appear in the wayback machine to ensure proper rendering. In addition to visual inspection, you will want to take a close look at crawl reports to gain better insight into the amount and types of data being collection.\n\n\nQualitative Analysis\n\n\nIt is important to maintain ongoing visual inspections of our archived seeds to ensure that they continue to render properly to visitors of the archive. After each recurring crawl (for monthly and quarterly seeds) you should inspect the home page and first level navigation items of the site. Ideally this should be done within about one week from the conclusion of the recurring crawl. The same type of inspection should be done on a monthly basis for the seeds that are in the daily and weekly crawl frequencies.\n\n\nQuantitative Analysis\n\n\nDue to the large number of seeds in our collection, we have to rely on quantitative data to better understand the efficacy of our crawls. Using the amount of data collected on any given crawl, and comparing it to other recurring crawl reports allows us to identify any outliers which may indicate an issue. For example if the monthly crawl generally returns about 6GB of data and then in one month jumps to 10GB there may be an issue, and you will need to closely inspect the crawl report.\n\n\nData Budget Management\n\n\nWe have a set data quota per year that we must remain within. This is why maintaining an active awareness of our current data budget, and the average data usage for recurring crawls is helpful. We have created \nthis Google Spreadsheet\n to help us track our data usage throughout the year.\n\n\nMajor Website Changes\n\n\nIt is important to document when a website we are currently crawling undergoes major changes or a complete redesign which could change the quality of the crawl. This will prompt the web archiving technician to complete a test crawl on the redesigned site, to ensure that regular recurring crawls will complete correctly.", 
            "title": "Recurring Crawl QA"
        }, 
        {
            "location": "/recurring-crawl-qa/#recurring-crawl-quality-assurance", 
            "text": "While most of the heavy quality assurance work occurs during the process of adding a new seed to the collection, certain steps must take place to ensure that recurring crawls are not encountering any errors. There are two main aspects to recurring crawl quality assurance. During the qualitative analysis phase you will want to visually inspect websites as they appear in the wayback machine to ensure proper rendering. In addition to visual inspection, you will want to take a close look at crawl reports to gain better insight into the amount and types of data being collection.", 
            "title": "Recurring Crawl Quality Assurance"
        }, 
        {
            "location": "/recurring-crawl-qa/#qualitative-analysis", 
            "text": "It is important to maintain ongoing visual inspections of our archived seeds to ensure that they continue to render properly to visitors of the archive. After each recurring crawl (for monthly and quarterly seeds) you should inspect the home page and first level navigation items of the site. Ideally this should be done within about one week from the conclusion of the recurring crawl. The same type of inspection should be done on a monthly basis for the seeds that are in the daily and weekly crawl frequencies.", 
            "title": "Qualitative Analysis"
        }, 
        {
            "location": "/recurring-crawl-qa/#quantitative-analysis", 
            "text": "Due to the large number of seeds in our collection, we have to rely on quantitative data to better understand the efficacy of our crawls. Using the amount of data collected on any given crawl, and comparing it to other recurring crawl reports allows us to identify any outliers which may indicate an issue. For example if the monthly crawl generally returns about 6GB of data and then in one month jumps to 10GB there may be an issue, and you will need to closely inspect the crawl report.", 
            "title": "Quantitative Analysis"
        }, 
        {
            "location": "/recurring-crawl-qa/#data-budget-management", 
            "text": "We have a set data quota per year that we must remain within. This is why maintaining an active awareness of our current data budget, and the average data usage for recurring crawls is helpful. We have created  this Google Spreadsheet  to help us track our data usage throughout the year.", 
            "title": "Data Budget Management"
        }, 
        {
            "location": "/recurring-crawl-qa/#major-website-changes", 
            "text": "It is important to document when a website we are currently crawling undergoes major changes or a complete redesign which could change the quality of the crawl. This will prompt the web archiving technician to complete a test crawl on the redesigned site, to ensure that regular recurring crawls will complete correctly.", 
            "title": "Major Website Changes"
        }, 
        {
            "location": "/searching/", 
            "text": "Searching the \nNCSU Libraries Collection\n\n\nThe websites that we have targeted and crawled by NCSU Libraries can be found at \nhttps://archive-it.org/home/ncsu-libraries\n. This page will show you a list of our collections, as well as each URL that we have targeted for crawling. There are two primary ways to search our collections page.\n\n\nSearching by Search Term.\n\n\nThe search box on \nour Archive-It page\n allows you to enter URLs or individual search terms. It works best with full URLs as individual search terms are likely to appear on too many websites to be relevant. The Archive-It service currently only indexes the home page of each capture of each seed URL meaning search terms that would be deep within any given site are somewhat unlikely to be found. The following tips may help with your search.\n\n\n\n\n\n\nWhen attempting to find an individual URL in our collection, paste the entire URL into the search box at \nhttps://archive-it.org/home/ncsu-libraries\n\n\n\n\n\n\nOnce results are returned clicking on the URL link in the search results will take you to the search results landing page. This page shows the date of every crawl of that URL. Dates with an asterisk next to them show that new data was collected with that crawl. Clicking on the date of the crawl will take you to the captured website from that crawl.\n\n\n\n\n\n\nSearching by URL\n\n\n\n\n\n\nYou can search by URL by entering the full URL of the website you hope to find in the general search bar on the main \nNCSU libraries Archive-It landing page\n. Using this search will likely return a large list of results (particularly within the NC State University Websites Collection) as nearly every URL includes NCSU somewhere in it.\n\n\n\n\n\n\nSearching from the URL Bar\n is perhaps the easiest way to search for websites that we have collected. You can enter the URL directly by using the following format for the URL: https://wayback.archive-it.org/org-972/*/FULL_URL_TO_Find For example pasting this url directly into your browser \nhttps://wayback.archive-it.org/org-972/*/https://ncsu.edu\n allows you to find each capture of the main NCSU website. You would replace https://ncsu.edu with any full URL that you wished to see if we had captured.\n\n\n\n\n\n\nSearching the General \nWayback Machine\n\n\nThe Wayback Machine provided by the Internet Archive allows anyone to search for any URL to see if it has been captured by the Internet Archive. The primary difference between this search box and our NCSU Libraries specific collection search box is that the public wayback requires a full URL for a search to work, it does not accept search terms.\n\n\n\n\n\n\nOnce you have put a URL into the search bar you will be taken to the landing page for that URL where it shows each date the URL was crawled and captured. Similar to the results you would get in our collection, clicking on a date will take you to the individual capture of the site.\n\n\n\n\n\n\nSearching from the URL Bar\n is perhaps the easiest way to search for websites that we have collected. You can enter the URL directly by using the following format for the URL - http://web.archive.org/web/\n/\n/FULL_URL_TO_Find. For example pasting this url directly into your browser \nhttp://web.archive.org/web/*/https://ncsu.edu\n allows you to find each capture of the main NCSU website. You would replace https://ncsu.edu with any full URL that you wished to see if we had captured.", 
            "title": "Using Our Web Archives"
        }, 
        {
            "location": "/searching/#searching-the-ncsu-libraries-collection", 
            "text": "The websites that we have targeted and crawled by NCSU Libraries can be found at  https://archive-it.org/home/ncsu-libraries . This page will show you a list of our collections, as well as each URL that we have targeted for crawling. There are two primary ways to search our collections page.", 
            "title": "Searching the NCSU Libraries Collection"
        }, 
        {
            "location": "/searching/#searching-by-search-term", 
            "text": "The search box on  our Archive-It page  allows you to enter URLs or individual search terms. It works best with full URLs as individual search terms are likely to appear on too many websites to be relevant. The Archive-It service currently only indexes the home page of each capture of each seed URL meaning search terms that would be deep within any given site are somewhat unlikely to be found. The following tips may help with your search.    When attempting to find an individual URL in our collection, paste the entire URL into the search box at  https://archive-it.org/home/ncsu-libraries    Once results are returned clicking on the URL link in the search results will take you to the search results landing page. This page shows the date of every crawl of that URL. Dates with an asterisk next to them show that new data was collected with that crawl. Clicking on the date of the crawl will take you to the captured website from that crawl.", 
            "title": "Searching by Search Term."
        }, 
        {
            "location": "/searching/#searching-by-url", 
            "text": "You can search by URL by entering the full URL of the website you hope to find in the general search bar on the main  NCSU libraries Archive-It landing page . Using this search will likely return a large list of results (particularly within the NC State University Websites Collection) as nearly every URL includes NCSU somewhere in it.    Searching from the URL Bar  is perhaps the easiest way to search for websites that we have collected. You can enter the URL directly by using the following format for the URL: https://wayback.archive-it.org/org-972/*/FULL_URL_TO_Find For example pasting this url directly into your browser  https://wayback.archive-it.org/org-972/*/https://ncsu.edu  allows you to find each capture of the main NCSU website. You would replace https://ncsu.edu with any full URL that you wished to see if we had captured.", 
            "title": "Searching by URL"
        }, 
        {
            "location": "/searching/#searching-the-general-wayback-machine", 
            "text": "The Wayback Machine provided by the Internet Archive allows anyone to search for any URL to see if it has been captured by the Internet Archive. The primary difference between this search box and our NCSU Libraries specific collection search box is that the public wayback requires a full URL for a search to work, it does not accept search terms.    Once you have put a URL into the search bar you will be taken to the landing page for that URL where it shows each date the URL was crawled and captured. Similar to the results you would get in our collection, clicking on a date will take you to the individual capture of the site.    Searching from the URL Bar  is perhaps the easiest way to search for websites that we have collected. You can enter the URL directly by using the following format for the URL - http://web.archive.org/web/ / /FULL_URL_TO_Find. For example pasting this url directly into your browser  http://web.archive.org/web/*/https://ncsu.edu  allows you to find each capture of the main NCSU website. You would replace https://ncsu.edu with any full URL that you wished to see if we had captured.", 
            "title": "Searching the General Wayback Machine"
        }, 
        {
            "location": "/resources/", 
            "text": "Archive-It Support Center\n\n\nIIPC Web Archiving Resources\n\n\nIIPC Web Archiving Collection Development Policies\n\n\nNYARC QA Guidelines\n\n\nHarvard Library's Environmental Scan of Web Archiving", 
            "title": "Other Resources"
        }, 
        {
            "location": "/resources/#archive-it-support-center", 
            "text": "", 
            "title": "Archive-It Support Center"
        }, 
        {
            "location": "/resources/#iipc-web-archiving-resources", 
            "text": "", 
            "title": "IIPC Web Archiving Resources"
        }, 
        {
            "location": "/resources/#iipc-web-archiving-collection-development-policies", 
            "text": "", 
            "title": "IIPC Web Archiving Collection Development Policies"
        }, 
        {
            "location": "/resources/#nyarc-qa-guidelines", 
            "text": "", 
            "title": "NYARC QA Guidelines"
        }, 
        {
            "location": "/resources/#harvard-librarys-environmental-scan-of-web-archiving", 
            "text": "", 
            "title": "Harvard Library's Environmental Scan of Web Archiving"
        }
    ]
}